#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Xå¹³å°æ¨æ–‡æ•°æ®è¿˜åŸè„šæœ¬
ä» tweets.json æ–‡ä»¶ä¸­è¿˜åŸæ•°æ®åˆ°æ•°æ®åº“
"""

import json
import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from base.database import DatabaseManager
from base.logger import get_logger


def load_tweets_from_json(json_file_path):
    """ä»JSONæ–‡ä»¶åŠ è½½æ¨æ–‡æ•°æ®"""
    logger = get_logger(__name__)
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            tweets = json.load(f)
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(tweets)} æ¡æ¨æ–‡æ•°æ®")
        return tweets
    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSONè§£æé”™è¯¯: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []


def convert_tweet_format(tweet_data):
    """å°†JSONæ ¼å¼çš„æ¨æ–‡æ•°æ®è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼"""
    try:
        # å¤„ç†å›¾ç‰‡åˆ—è¡¨
        images = tweet_data.get('images', [])
        images_str = ','.join(images) if images else None

        # å¤„ç†è§†é¢‘åˆ—è¡¨
        videos = tweet_data.get('videos', [])
        videos_str = ','.join(videos) if videos else None

        # è½¬æ¢æ—¶é—´æ ¼å¼
        publish_time_str = tweet_data.get('publishTime', '')
        if publish_time_str:
            # è§£æISOæ ¼å¼æ—¶é—´
            publish_time = datetime.fromisoformat(publish_time_str.replace('Z', '+00:00'))
        else:
            publish_time = datetime.now()

        # ä»tweetUrlæå–tweetId
        tweet_url = tweet_data.get('tweetUrl', '')
        tweet_id = None
        if tweet_url and '/status/' in tweet_url:
            tweet_id = tweet_url.split('/status/')[-1].split('?')[0]

        return {
            'screenName': tweet_data.get('screenName', ''),
            'images': images_str,
            'videos': videos_str,
            'tweetUrl': tweet_url,
            'fullText': tweet_data.get('fullText', ''),
            'publishTime': publish_time,
            'tweetId': tweet_id
        }
    except Exception as e:
        logger = get_logger(__name__)
        logger.error(f"âŒ è½¬æ¢æ¨æ–‡æ•°æ®æ—¶å‡ºé”™: {e}")
        logger.error(f"   åŸå§‹æ•°æ®: {tweet_data}")
        return None


def restore_tweets_to_database(tweets_data):
    """å°†æ¨æ–‡æ•°æ®è¿˜åŸåˆ°æ•°æ®åº“"""
    logger = get_logger(__name__)
    db = DatabaseManager()

    success_count = 0
    error_count = 0
    duplicate_count = 0

    logger.info(f"ğŸš€ å¼€å§‹è¿˜åŸ {len(tweets_data)} æ¡æ¨æ–‡åˆ°æ•°æ®åº“...")

    for i, tweet_data in enumerate(tweets_data, 1):
        try:
            # è½¬æ¢æ•°æ®æ ¼å¼
            converted_tweet = convert_tweet_format(tweet_data)
            if not converted_tweet:
                error_count += 1
                continue

            # ä¿å­˜åˆ°æ•°æ®åº“
            result = db.save_tweet(converted_tweet)

            if result:
                success_count += 1
                if i % 100 == 0:
                    logger.info(
                        f"ğŸ“Š è¿›åº¦: {i}/{len(tweets_data)} - æˆåŠŸ: {success_count}, é‡å¤: {duplicate_count}, é”™è¯¯: {error_count}")
            else:
                duplicate_count += 1

        except Exception as e:
            error_count += 1
            logger.error(f"âŒ å¤„ç†ç¬¬ {i} æ¡æ¨æ–‡æ—¶å‡ºé”™: {e}")
            continue

    logger.info(f"âœ… æ•°æ®è¿˜åŸå®Œæˆ!")
    logger.info(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    logger.info(f"   - æ€»æ•°æ®é‡: {len(tweets_data)}")
    logger.info(f"   - æˆåŠŸå¯¼å…¥: {success_count}")
    logger.info(f"   - é‡å¤è·³è¿‡: {duplicate_count}")
    logger.info(f"   - é”™è¯¯æ•°é‡: {error_count}")

    return success_count, duplicate_count, error_count


def main():
    """ä¸»å‡½æ•°"""
    logger = get_logger(__name__)
    logger.info("ğŸ”„ Xå¹³å°æ¨æ–‡æ•°æ®è¿˜åŸå·¥å…·")
    logger.info("=" * 50)

    # JSONæ–‡ä»¶è·¯å¾„
    json_file_path = "data/x/tweets.json"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_file_path):
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        return

    # åŠ è½½æ¨æ–‡æ•°æ®
    tweets_data = load_tweets_from_json(json_file_path)
    if not tweets_data:
        logger.error("âŒ æ²¡æœ‰å¯è¿˜åŸçš„æ•°æ®")
        return

    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    logger.info(f"ğŸ“‹ æ•°æ®é¢„è§ˆ (å‰3æ¡):")
    for i, tweet in enumerate(tweets_data[:3], 1):
        logger.info(f"  {i}. {tweet.get('screenName', 'Unknown')} - {tweet.get('publishTime', 'Unknown')}")
        logger.info(f"     {tweet.get('fullText', '')[:50]}...")
        if tweet.get('images'):
            logger.info(f"     ğŸ“¸ å›¾ç‰‡: {len(tweet['images'])}å¼ ")
        if tweet.get('videos'):
            logger.info(f"     ğŸ¥ è§†é¢‘: {len(tweet['videos'])}ä¸ª")

    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    confirm = input(f"æ˜¯å¦ç»§ç»­è¿˜åŸ {len(tweets_data)} æ¡æ¨æ–‡åˆ°æ•°æ®åº“? (y/N): ").strip().lower()
    if confirm != 'y':
        logger.warning("âŒ æ“ä½œå·²å–æ¶ˆ")
        return

    # æ‰§è¡Œæ•°æ®è¿˜åŸ
    restore_tweets_to_database(tweets_data)

    logger.info(f"ğŸ‰ æ•°æ®è¿˜åŸä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()
