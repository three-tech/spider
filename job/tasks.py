"""
Xå¹³å°çˆ¬è™«å®šæ—¶ä»»åŠ¡å…·ä½“å®ç°
"""

import json
import os
import sys
from base.logger import get_logger
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥baseæ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# æ·»åŠ xæ¨¡å—è·¯å¾„
x_module_path = os.path.join(project_root, 'x')
sys.path.insert(0, x_module_path)

# å¯¼å…¥æ¨¡å—
from base.database import DatabaseManager, MemberXhs
from base.logger import get_logger
from base.utils import ImageDownloadManager
from sms.notification_manager import get_notification_manager
from x.x_spider_optimized import XSpiderOptimized


def crawl_followed_users_task(config_path: str = None):
    """
    çˆ¬å–å…³æ³¨ç”¨æˆ·æ¨æ–‡çš„å®šæ—¶ä»»åŠ¡
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: ä»»åŠ¡æ‰§è¡Œç»“æœ
    """
    logger = get_logger(__name__)

    try:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        if config_path is None:
            # è·å–é¡¹ç›®æ ¹ç›®å½•çš„é…ç½®æ–‡ä»¶
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            config_path = os.path.join(project_root, 'config.json')

        # è¯»å–é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        db = DatabaseManager()

        # è·å–å…³æ³¨çš„ç”¨æˆ·åˆ—è¡¨
        followed_users = db.get_followed_users()

        if not followed_users:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å…³æ³¨çš„ç”¨æˆ·")
            return {
                'success': True,
                'stats': {'total_users': 0, 'total_tweets': 0},
                'message': 'æ²¡æœ‰å…³æ³¨çš„ç”¨æˆ·'
            }

        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(followed_users)} ä¸ªå…³æ³¨çš„ç”¨æˆ·")

        # åˆå§‹åŒ–çˆ¬è™«
        spider = XSpiderOptimized(config_path)

        total_tweets = 0
        successful_users = 0
        failed_users = []

        # éå†æ¯ä¸ªå…³æ³¨çš„ç”¨æˆ·
        for user in followed_users:
            screen_name = user.screen_name

            try:
                logger.info(f"ğŸ” å¼€å§‹çˆ¬å–ç”¨æˆ·: @{screen_name}")

                # è·å–ç”¨æˆ·æœ€åçˆ¬å–ä¿¡æ¯
                crawl_info = db.get_user_last_crawl_info(screen_name)

                # è®¾ç½®çˆ¬å–å‚æ•°
                crawl_params = {
                    'screen_name': screen_name,
                    'count': config.get('crawl_count', 20),  # é»˜è®¤çˆ¬å–20æ¡
                    'include_rts': config.get('include_retweets', True)
                }

                # å¦‚æœæœ‰ä¸Šæ¬¡çˆ¬å–æ—¶é—´ï¼Œè®¾ç½®since_timeè¿›è¡Œå¢é‡çˆ¬å–
                if crawl_info and crawl_info.get('last_tweet_time'):
                    # ä»ä¸Šæ¬¡æœ€æ–°æ¨æ–‡æ—¶é—´å¼€å§‹çˆ¬å–
                    crawl_params['since_time'] = crawl_info['last_tweet_time']
                    logger.info(f"ğŸ“… å¢é‡çˆ¬å–ï¼Œä» {crawl_info['last_tweet_time']} å¼€å§‹")
                else:
                    # é¦–æ¬¡çˆ¬å–ï¼Œè·å–æœ€è¿‘7å¤©çš„æ¨æ–‡
                    since_time = datetime.now() - timedelta(days=7)
                    crawl_params['since_time'] = since_time
                    logger.info(f"ğŸ“… é¦–æ¬¡çˆ¬å–ï¼Œè·å–æœ€è¿‘7å¤©æ¨æ–‡")

                # æ‰§è¡Œçˆ¬å– - ä½¿ç”¨ process_user_tweets æ–¹æ³•
                tweets = spider.process_user_tweets(screen_name)

                if tweets:
                    # process_user_tweets å·²ç»åŒ…å«ä¿å­˜é€»è¾‘ï¼Œç›´æ¥ç»Ÿè®¡æ•°é‡
                    total_tweets += len(tweets)

                    logger.info(f"âœ… @{screen_name}: å¤„ç†äº† {len(tweets)} æ¡æ¨æ–‡")
                    successful_users += 1
                else:
                    logger.info(f"â„¹ï¸ @{screen_name}: æ²¡æœ‰æ–°æ¨æ–‡")
                    successful_users += 1

            except Exception as e:
                logger.error(f"âŒ çˆ¬å–ç”¨æˆ· @{screen_name} å¤±è´¥: {e}")
                failed_users.append({
                    'screen_name': screen_name,
                    'error': str(e)
                })

        # å…³é—­èµ„æº - XSpiderOptimized ä¸éœ€è¦æ˜¾å¼å…³é—­
        # spider.close()  # XSpiderOptimized æ²¡æœ‰ close æ–¹æ³•
        # db.close()      # DatabaseManager ä¹Ÿä¸éœ€è¦æ˜¾å¼å…³é—­

        # ç»Ÿè®¡ç»“æœ
        stats = {
            'total_users': len(followed_users),
            'successful_users': successful_users,
            'failed_users': len(failed_users),
            'total_tweets': total_tweets,
            'failed_user_list': failed_users
        }

        logger.info(f"ğŸ“Š ä»»åŠ¡å®Œæˆç»Ÿè®¡:")
        logger.info(f"   ğŸ‘¥ æ€»ç”¨æˆ·æ•°: {stats['total_users']}")
        logger.info(f"   âœ… æˆåŠŸ: {stats['successful_users']}")
        logger.info(f"   âŒ å¤±è´¥: {stats['failed_users']}")
        logger.info(f"   ğŸ“ æ€»æ¨æ–‡æ•°: {stats['total_tweets']}")

        return {
            'success': True,
            'stats': stats,
            'message': f'æˆåŠŸçˆ¬å– {successful_users}/{len(followed_users)} ä¸ªç”¨æˆ·ï¼Œå…± {total_tweets} æ¡æ¨æ–‡'
        }

    except Exception as e:
        logger.error(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'stats': {}
        }


def cleanup_old_tweets_task(days=30):
    """
    æ¸…ç†æ—§æ¨æ–‡çš„ä»»åŠ¡
    
    Args:
        days: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤30å¤©
        
    Returns:
        dict: æ¸…ç†ç»“æœ
    """
    logger = get_logger(__name__)

    try:
        db = DatabaseManager()

        # è®¡ç®—æˆªæ­¢æ—¥æœŸ
        cutoff_date = datetime.now() - timedelta(days=days)

        # è¿™é‡Œå¯ä»¥å®ç°æ¸…ç†é€»è¾‘
        # ä¾‹å¦‚ï¼šåˆ é™¤è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ¨æ–‡
        logger.info(f"ğŸ§¹ æ¸…ç† {cutoff_date} ä¹‹å‰çš„æ¨æ–‡")

        # å®é™…æ¸…ç†ä»£ç ...

        db.close()

        return {
            'success': True,
            'message': f'æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cutoff_date} ä¹‹å‰çš„æ¨æ–‡'
        }

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


def backup_database_task():
    """
    æ•°æ®åº“å¤‡ä»½ä»»åŠ¡
    
    Returns:
        dict: å¤‡ä»½ç»“æœ
    """
    logger = get_logger(__name__)

    try:
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®åº“å¤‡ä»½é€»è¾‘
        backup_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"backup_x_spider_{backup_time}.sql"

        logger.info(f"ğŸ’¾ å¼€å§‹æ•°æ®åº“å¤‡ä»½: {backup_file}")

        # å®é™…å¤‡ä»½ä»£ç ...

        return {
            'success': True,
            'message': f'æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}'
        }

    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


def xhs_auto_publish_task(config_path='config.json'):
    """
    å°çº¢ä¹¦è‡ªåŠ¨å‘å¸ƒä»»åŠ¡
    
    åŠŸèƒ½æµç¨‹ï¼š
    1. è·å–member_xhsè¡¨çš„æ‰€æœ‰è´¦æˆ·
    2. æ ¹æ®xhs tagsæ‰¾åˆ°ç›¸åŒæ ‡ç­¾çš„xè´¦æˆ·
    3. ä»resource_xä¸­è·å–æœªå‘é€è¿‡çš„æ¨æ–‡
    4. è°ƒç”¨XiaoHongShuImgçš„mainæ–¹æ³•å‘å¸ƒ
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: ä»»åŠ¡æ‰§è¡Œç»“æœ
    """
    logger = get_logger(__name__)

    try:
        # ç¡®ä¿é…ç½®æ–‡ä»¶è·¯å¾„æ­£ç¡®
        if not os.path.isabs(config_path):
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), config_path)

        # è¯»å–é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        db = DatabaseManager()

        # è·å–æ‰€æœ‰å°çº¢ä¹¦ä¼šå‘˜
        xhs_members = db.get_all_member_xhs()

        if not xhs_members:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å°çº¢ä¹¦ä¼šå‘˜")
            return {
                'success': True,
                'stats': {'total_xhs_members': 0, 'published_count': 0},
                'message': 'æ²¡æœ‰å°çº¢ä¹¦ä¼šå‘˜'
            }

        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(xhs_members)} ä¸ªå°çº¢ä¹¦ä¼šå‘˜")

        total_published = 0
        successful_members = 0
        failed_members = []

        # éå†æ¯ä¸ªå°çº¢ä¹¦ä¼šå‘˜
        for xhs_member in xhs_members:
            # MemberXhs æ˜¯ SQLAlchemy å¯¹è±¡ï¼Œç›´æ¥è®¿é—®å±æ€§
            xhs_id = xhs_member.xhs_id
            user_name = xhs_member.userName
            xhs_tags = xhs_member.tags or '' if hasattr(xhs_member, 'tags') else ''

            try:
                logger.info(f"ğŸ” å¤„ç†å°çº¢ä¹¦ä¼šå‘˜: {user_name} (ID: {xhs_id})")

                if not xhs_tags:
                    logger.warning(f"âš ï¸ ä¼šå‘˜ {user_name} æ²¡æœ‰è®¾ç½®æ ‡ç­¾ï¼Œè·³è¿‡")
                    continue

                # è§£ææ ‡ç­¾åˆ—è¡¨
                tag_list = [tag.strip() for tag in xhs_tags.split(',') if tag.strip()]
                logger.info(f"ğŸ·ï¸ ä¼šå‘˜æ ‡ç­¾: {tag_list}")

                # æ ¹æ®æ ‡ç­¾æ‰¾åˆ°ç›¸åŒæ ‡ç­¾çš„Xè´¦æˆ·
                matching_x_users = []
                for tag in tag_list:
                    # åœ¨member_xè¡¨ä¸­æœç´¢åŒ…å«ç›¸åŒæ ‡ç­¾çš„ç”¨æˆ·
                    x_users = db.search_members_by_tag(tag)  # è¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨DatabaseManagerä¸­å®ç°
                    matching_x_users.extend(x_users)

                # å»é‡
                unique_x_users = {}
                for user in matching_x_users:
                    # æ£€æŸ¥ user æ˜¯å­—å…¸è¿˜æ˜¯å¯¹è±¡
                    if hasattr(user, 'screen_name'):
                        unique_x_users[user.screen_name] = user
                    else:
                        unique_x_users[user.screen_name] = user
                matching_x_users = list(unique_x_users.values())

                if not matching_x_users:
                    logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä¸æ ‡ç­¾ {tag_list} åŒ¹é…çš„Xè´¦æˆ·")
                    continue

                logger.info(f"ğŸ‘¥ æ‰¾åˆ° {len(matching_x_users)} ä¸ªåŒ¹é…çš„Xè´¦æˆ·")

                # ä»åŒ¹é…çš„Xè´¦æˆ·ä¸­è·å–æœªå‘é€è¿‡çš„æ¨æ–‡
                matching_screen_names = []
                for user in matching_x_users:
                    if hasattr(user, 'screen_name'):
                        matching_screen_names.append(user.screen_name)
                    else:
                        matching_screen_names.append(user.screen_name)
                unpublished_tweet = db.get_unpublished_tweet_by_xhs_member(xhs_id, matching_screen_names)

                if unpublished_tweet:
                    # æ£€æŸ¥ unpublished_tweet æ˜¯å­—å…¸è¿˜æ˜¯å¯¹è±¡
                    if hasattr(unpublished_tweet, 'screenName'):
                        logger.info(f"ğŸ“ æ‰¾åˆ°æœªå‘å¸ƒæ¨æ–‡ï¼Œæ¥è‡ªç”¨æˆ·: @{unpublished_tweet.screenName}")
                    logger.info(f"ğŸ“ æ‰¾åˆ°æœªå‘å¸ƒæ¨æ–‡ï¼Œæ¥è‡ªç”¨æˆ·: @{unpublished_tweet.screenName}")

                if not unpublished_tweet:
                    logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœªå‘å¸ƒçš„æ¨æ–‡")
                    continue

                # å‡†å¤‡å‘å¸ƒå†…å®¹
                if hasattr(unpublished_tweet, 'fullText'):
                    title = unpublished_tweet.fullText[:50] + "..." if unpublished_tweet.fullText else ""
                    content = unpublished_tweet.fullText or ""
                title = unpublished_tweet.fullText[:50] + "..." if unpublished_tweet.fullText else ""
                content = unpublished_tweet.fullText or ""

                # å¤„ç†å›¾ç‰‡è·¯å¾„ - ä¸‹è½½åˆ°æœ¬åœ°
                if hasattr(unpublished_tweet, 'images'):
                    images = unpublished_tweet.images or ""
                images = unpublished_tweet.images or ""
                if not images:
                    logger.warning(f"âš ï¸ æ¨æ–‡æ²¡æœ‰å›¾ç‰‡ï¼Œè·³è¿‡å‘å¸ƒ")
                    continue

                # è§£æå›¾ç‰‡URLåˆ—è¡¨
                image_urls = [url.strip() for url in images.split(',') if url.strip()]
                if not image_urls:
                    logger.warning(f"âš ï¸ æ¨æ–‡å›¾ç‰‡URLæ— æ•ˆï¼Œè·³è¿‡å‘å¸ƒ")
                    continue

                logger.info(f"ğŸ“¸ å¼€å§‹ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡...")

                # ä½¿ç”¨å›¾ç‰‡ä¸‹è½½ç®¡ç†å™¨ - å®Œæ•´çš„ä¸‹è½½-å‘å¸ƒ-æ¸…ç†æµç¨‹
                with ImageDownloadManager() as img_manager:
                    # ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
                    local_image_paths = img_manager.download_images(image_urls, max_images=9)

                    if not local_image_paths:
                        logger.warning(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡å‘å¸ƒ")
                        continue

                    logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(local_image_paths)} å¼ å›¾ç‰‡")

                    # å‡†å¤‡æœ¬åœ°æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
                    file_path = ','.join(local_image_paths)

                    # å‡†å¤‡æ ‡ç­¾
                    publish_tags = tag_list[:10]  # å°çº¢ä¹¦æ ‡ç­¾é™åˆ¶

                    # å‘å¸ƒæ—¶é—´è®¾ç½®ä¸ºå½“å‰æ—¶é—´
                    publish_date = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')

                    # å‘é€é£ä¹¦é€šçŸ¥
                    try:
                        notification_manager = get_notification_manager()
                        if notification_manager.is_notification_enabled():
                            logger.info("ğŸ“¤ å‘é€é£ä¹¦é€šçŸ¥...")

                            # å‡†å¤‡é€šçŸ¥ä¿¡æ¯
                            if hasattr(unpublished_tweet, 'publishTime'):
                                tweet_publish_time = unpublished_tweet.publishTime
                                tweet_content = unpublished_tweet.fullText or ""
                                tweet_author = unpublished_tweet.screenName
                            tweet_publish_time = unpublished_tweet.publishTime
                            tweet_content = unpublished_tweet.fullText or ""
                            tweet_author = unpublished_tweet.screenName

                            # å‘é€é€šçŸ¥
                            notification_result = notification_manager.send_xhs_publish_notification(
                                xhs_account=str(user_name),
                                image_count=len(image_urls),
                                image_paths=image_urls,
                                tweet_publish_time=str(tweet_publish_time),
                                tweet_content=str(tweet_content),
                                tweet_author=str(tweet_author)
                            )

                            if notification_result.get('feishu', {}).get('success'):
                                logger.info("âœ… é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
                            else:
                                logger.warning("âš ï¸ é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥")
                        else:
                            logger.info("â„¹ï¸ é€šçŸ¥åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡é€šçŸ¥")
                    except Exception as notify_error:
                        logger.error(f"âŒ å‘é€é€šçŸ¥æ—¶å‡ºé”™: {notify_error}")

                    logger.info(f"ğŸš€ å¼€å§‹å‘å¸ƒåˆ°å°çº¢ä¹¦...")
                    logger.info(f"   æ ‡é¢˜: {title}")
                    logger.info(f"   æ ‡ç­¾: {publish_tags}")
                    logger.info(f"   å›¾ç‰‡: {len(local_image_paths)} å¼ ")

                    # è°ƒç”¨XiaoHongShuImgçš„mainæ–¹æ³•
                    # æ³¨æ„ï¼šè¿™æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦åœ¨åŒæ­¥ç¯å¢ƒä¸­è°ƒç”¨
                    import asyncio
                    from xiaohongshu.xhs_upload_img import XiaoHongShuImg

                    xhs_uploader = XiaoHongShuImg(
                        user_name=str(user_name),
                        title=title,
                        file_path=file_path,
                        tags=publish_tags,
                        member_xhs=xhs_member,
                        publish_date=publish_date,
                        content=str(tweet_content),
                        headless=False  # åå°è¿è¡Œ
                    )

                    # åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥æ–¹æ³•
                    try:
                        asyncio.run(xhs_uploader.main())

                        # æ ‡è®°æ¨æ–‡ä¸ºå·²å‘å¸ƒ
                        if hasattr(unpublished_tweet, 'id'):
                            db.mark_tweet_as_published(unpublished_tweet.id, xhs_id)
                        db.mark_tweet_as_published(unpublished_tweet.id, xhs_id)

                        total_published += 1
                        successful_members += 1

                        logger.info(f"âœ… {user_name}: æˆåŠŸå‘å¸ƒæ¨æ–‡")
                        logger.info(f"ğŸ§¹ å›¾ç‰‡å°†åœ¨é€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†")

                    except Exception as upload_error:
                        logger.error(f"âŒ å‘å¸ƒå¤±è´¥: {upload_error}")
                        failed_members.append({
                            'xhs_id': xhs_id,
                            'user_name': user_name,
                            'error': str(upload_error)
                        })
                # æ³¨æ„ï¼šå›¾ç‰‡æ–‡ä»¶ä¼šåœ¨withå—ç»“æŸæ—¶è‡ªåŠ¨åˆ é™¤

            except Exception as e:
                logger.error(f"âŒ å¤„ç†ä¼šå‘˜ {user_name} å¤±è´¥: {e}")
                failed_members.append({
                    'xhs_id': xhs_id,
                    'user_name': user_name,
                    'error': str(e)
                })

        # ç»Ÿè®¡ç»“æœ
        stats = {
            'total_xhs_members': len(xhs_members),
            'successful_members': successful_members,
            'failed_members': len(failed_members),
            'published_count': total_published,
            'failed_member_list': failed_members
        }

        logger.info(f"ğŸ“Š å°çº¢ä¹¦å‘å¸ƒä»»åŠ¡å®Œæˆç»Ÿè®¡:")
        logger.info(f"   ğŸ‘¥ æ€»ä¼šå‘˜æ•°: {stats['total_xhs_members']}")
        logger.info(f"   âœ… æˆåŠŸ: {stats['successful_members']}")
        logger.info(f"   âŒ å¤±è´¥: {stats['failed_members']}")
        logger.info(f"   ğŸ“ å‘å¸ƒæ•°: {stats['published_count']}")

        return {
            'success': True,
            'stats': stats,
            'message': f'æˆåŠŸå¤„ç† {successful_members}/{len(xhs_members)} ä¸ªä¼šå‘˜ï¼Œå‘å¸ƒ {total_published} æ¡å†…å®¹'
        }

    except Exception as e:
        logger.error(f"âŒ å°çº¢ä¹¦å‘å¸ƒä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'stats': {}
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»»åŠ¡æ‰§è¡Œ
    logger = get_logger(__name__)
    logger.info("ğŸ§ª æµ‹è¯•å®šæ—¶ä»»åŠ¡...")
    result = crawl_followed_users_task()
    logger.info(f"ğŸ“Š ä»»åŠ¡ç»“æœ: {result}")
